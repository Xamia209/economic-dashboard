import json
import os
import unicodedata
from nltk.sentiment import SentimentIntensityAnalyzer
from collections import Counter

# =====================
# LOAD DATA
# =====================
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
input_path = os.path.join(BASE_DIR, "clean_data.json")

with open(input_path, "r", encoding="utf-8") as f:
    articles = json.load(f)

sia = SentimentIntensityAnalyzer()
results = []

# =====================
# SECTOR KEYWORDS
# =====================
SECTORS = {
    "banking": [
        "ngÃ¢n hÃ ng", "nhnn", "lÃ£i suáº¥t", "tÃ­n dá»¥ng", "vay",
        "lÃ£i Ä‘iá»u hÃ nh", "thanh khoáº£n", "huy Ä‘á»™ng vá»‘n"
    ],
    "real_estate": [
        "báº¥t Ä‘á»™ng sáº£n", "Ä‘á»‹a á»‘c", "nhÃ  Ä‘áº¥t", "chung cÆ°",
        "dá»± Ã¡n", "thá»‹ trÆ°á»ng nhÃ ", "mua bÃ¡n nhÃ "
    ],
    "stock": [
        "chá»©ng khoÃ¡n", "cá»• phiáº¿u", "vn-index", "vnindex",
        "hose", "hnx", "upcom", "thá»‹ trÆ°á»ng chá»©ng khoÃ¡n"
    ],
    "export": [
        "xuáº¥t kháº©u", "xuáº¥t nháº­p kháº©u", "Ä‘Æ¡n hÃ ng",
        "thÆ°Æ¡ng máº¡i", "kim ngáº¡ch", "fdi", "xuáº¥t sang"
    ],
    "macro": [
        "kinh táº¿", "tÄƒng trÆ°á»Ÿng", "láº¡m phÃ¡t", "gdp",
        "chÃ­nh sÃ¡ch", "vÄ© mÃ´", "tÃ i khÃ³a", "tiá»n tá»‡"
    ]
}

# =====================
# NORMALIZE TEXT (Cá»°C Ká»² QUAN TRá»ŒNG)
# =====================
def normalize_text(text: str) -> str:
    if not text:
        return ""
    text = unicodedata.normalize("NFC", text)
    return text.lower().strip()

def detect_sector(text):
    text = normalize_text(text)

    score = {}
    for sector, keywords in SECTORS.items():
        score[sector] = sum(
            1 for kw in keywords
            if normalize_text(kw) in text
        )

    best_sector = max(score, key=score.get)

    if score[best_sector] == 0:
        return "other"

    return best_sector

# =====================
# ANALYZE ARTICLES
# =====================
for article in articles:
    title = article.get("title", "")
    description = article.get("description", "")

    text = f"{title} {description}"

    sentiment_score = sia.polarity_scores(text)
    compound = sentiment_score["compound"]

    if compound >= 0.05:
        label = "positive"
    elif compound <= -0.05:
        label = "negative"
    else:
        label = "neutral"

    article["sentiment"] = sentiment_score
    article["sentiment_label"] = label
    article["sector"] = detect_sector(text)

    results.append(article)

# =====================
# SAVE sentiment_news.json
# =====================
with open(os.path.join(BASE_DIR, "sentiment_news.json"), "w", encoding="utf-8") as f:
    json.dump(results, f, ensure_ascii=False, indent=4)

print("ðŸ”¥ Sentiment analysis done!")

# =====================
# SECTOR SUMMARY
# =====================
sector_sentiment = {}

for article in results:
    sector = article.get("sector", "other")
    sentiment = article.get("sentiment_label", "neutral")

    sector_sentiment.setdefault(sector, Counter())
    sector_sentiment[sector][sentiment] += 1

summary = {
    sector: {
        "total": sum(counter.values()),
        "positive": counter.get("positive", 0),
        "neutral": counter.get("neutral", 0),
        "negative": counter.get("negative", 0),
    }
    for sector, counter in sector_sentiment.items()
}

with open(os.path.join(BASE_DIR, "sector_sentiment_summary.json"), "w", encoding="utf-8") as f:
    json.dump(summary, f, ensure_ascii=False, indent=4)

print("âœ… Saved sector sentiment summary")
